{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":13177,"status":"ok","timestamp":1706852961236,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"9DAZaMFnNc9B"},"outputs":[],"source":["!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q #transformer dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1766,"status":"ok","timestamp":1706852962992,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"PyPenjcaOJ1S","outputId":"e994d93b-a7ff-4281-9c44-4d9d24dcea15"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Feb  2 05:49:20 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   58C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi # to check which gpu we got"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12462,"status":"ok","timestamp":1706852975441,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"eDDWOfN6OWX_","outputId":"a63861b2-9b05-4599-82e0-d47a9473fbf8"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline, set_seed\n","\n","import matplotlib.pyplot as plt\n","from datasets import load_dataset\n","import pandas as pd\n","from datasets import load_dataset, load_metric\n","\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer #hugging face library\n","\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","from tqdm import tqdm\n","import torch\n","\n","nltk.download(\"punkt\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1706852975442,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"5ebuIezFOoFZ","outputId":"e3b0809f-3a8c-4c59-adf1-d75d20e60e45"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cuda'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device #cuda if gpu is selected"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5376,"status":"ok","timestamp":1706852980796,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"c33QYQlLPXtA","outputId":"c585efc8-514d-47d9-c54e-e63e77963639"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["model_ckpt = 'google/pegasus-cnn_dailymail'\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21912,"status":"ok","timestamp":1706853002699,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"N90azQEEPsZd","outputId":"1b2d9783-515a-46b9-8a8a-f91dc6773614"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device) #good prac to setting the device"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3020,"status":"ok","timestamp":1706853005682,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"YW5iV68_QSZQ"},"outputs":[],"source":["#load data\n","dataset_samsum = load_dataset(\"samsum\")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1706853005684,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"x-dcTyC8Qw1t","outputId":"2120bae8-90ff-4ae6-eba5-ebb35092b15c"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'summary'],\n","        num_rows: 14732\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'summary'],\n","        num_rows: 819\n","    })\n","    validation: Dataset({\n","        features: ['id', 'dialogue', 'summary'],\n","        num_rows: 818\n","    })\n","})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["dataset_samsum"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":46,"status":"ok","timestamp":1706853005684,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"7Td33Y0xQ3df"},"outputs":[],"source":["split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45,"status":"ok","timestamp":1706853005685,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"KpJa9VCkRPom","outputId":"3c8250b8-0a1f-4143-d01f-d6d56a8a8cf2"},"outputs":[{"data":{"text/plain":["[14732, 819, 818]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["split_lengths"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1706853005685,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"GXGTOh-TRRSd","outputId":"b67f0075-61a9-4db1-f879-0e8c7427d6bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Features: ['id', 'dialogue', 'summary']\n","\n","Example:\n","\n","   Dialogue:\n","Eric: MACHINE!\r\n","Rob: That's so gr8!\r\n","Eric: I know! And shows how Americans see Russian ;)\r\n","Rob: And it's really funny!\r\n","Eric: I know! I especially like the train part!\r\n","Rob: Hahaha! No one talks to the machine like that!\r\n","Eric: Is this his only stand-up?\r\n","Rob: Idk. I'll check.\r\n","Eric: Sure.\r\n","Rob: Turns out no! There are some of his stand-ups on youtube.\r\n","Eric: Gr8! I'll watch them now!\r\n","Rob: Me too!\r\n","Eric: MACHINE!\r\n","Rob: MACHINE!\r\n","Eric: TTYL?\r\n","Rob: Sure :)\n","\n","   Summary:\n","Eric and Rob are going to watch a stand-up on youtube.\n"]}],"source":["print(f\"Features: {dataset_samsum['train'].column_names}\")\n","print(\"\\nExample:\")\n","print(\"\\n   Dialogue:\")\n","\n","print(dataset_samsum[\"test\"][1][\"dialogue\"])\n","\n","print(\"\\n   Summary:\")\n","\n","print(dataset_samsum[\"test\"][1][\"summary\"])"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1706853005686,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"B7FdWd-YRgvN"},"outputs":[],"source":["#evaluating pegagus on samsum\n","dialogue = dataset_samsum[\"test\"][0]['dialogue']"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1706853005686,"user":{"displayName":"Kartik Gawande","userId":"12373185382785816463"},"user_tz":-330},"id":"GTBICSYoSBs-","outputId":"bfdeebcc-2813-49af-c3cc-c3d79a44b8a1"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Hannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nHannah: \u003cfile_gif\u003e\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nHannah: \u003cfile_gif\u003e\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him 🙂\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\""]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["dialogue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3luTqWMSWRN"},"outputs":[],"source":["pipe = pipeline('summarization', model = model_ckpt)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VfdQRj-nSghV"},"outputs":[{"name":"stderr","output_type":"stream","text":["Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"]},{"name":"stdout","output_type":"stream","text":["Amanda: Ask Larry Amanda: He called her last time we were at the park together \n","Hannah: I'd rather you texted him \n","Amanda: Just text him .\n"]}],"source":["pipe_out = pipe(dialogue)\n","print(pipe_out[0]['summary_text'].replace(\".\u003cn\u003e\",\"\\n\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dUKKEoBoUqof"},"outputs":[],"source":["\n","def generate_batch_sized_chunks(list_of_elements, batch_size):\n","    \"\"\"split the dataset into smaller batches that we can process simultaneously\n","    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n","    for i in range(0, len(list_of_elements), batch_size):\n","        yield list_of_elements[i : i + batch_size]\n","\n","def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n","                               batch_size=16, device=device,\n","                               column_text=\"article\",\n","                               column_summary=\"highlights\"):\n","    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n","    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n","\n","    for article_batch, target_batch in tqdm(\n","        zip(article_batches, target_batches), total=len(article_batches)):\n","\n","        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n","                        padding=\"max_length\", return_tensors=\"pt\")\n","\n","        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n","                         attention_mask=inputs[\"attention_mask\"].to(device),\n","                         length_penalty=0.8, num_beams=8, max_length=128)\n","        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n","\n","        # Finally, we decode the generated texts,\n","        # replace the  token, and add the decoded texts with the references to the metric.\n","        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n","                                clean_up_tokenization_spaces=True)\n","               for s in summaries]\n","\n","        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n","\n","\n","        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n","\n","    #  Finally compute and return the ROUGE scores.\n","    score = metric.compute()\n","    return score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjP5hasQSmHC"},"outputs":[],"source":["#summary aint good so we need to fine tune\n","rouge_metric = load_metric('rouge')\n","score = calculate_metric_on_test_ds(dataset_samsum['test'], rouge_metric, model_pegasus, tokenizer, column_text = 'dialogue', column_summary='summary', batch_size=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ayWgEaAgVtPi"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP440NyV+iVIKVBHNMH5nVn","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}